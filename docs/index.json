[{"body":"","link":"https://peerasan.github.io/","section":"","tags":null,"title":""},{"body":"","link":"https://peerasan.github.io/tags/ai/","section":"tags","tags":null,"title":"AI"},{"body":"","link":"https://peerasan.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://peerasan.github.io/tags/cline/","section":"tags","tags":null,"title":"Cline"},{"body":"","link":"https://peerasan.github.io/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"https://peerasan.github.io/tags/ollama/","section":"tags","tags":null,"title":"Ollama"},{"body":"","link":"https://peerasan.github.io/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://peerasan.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://peerasan.github.io/categories/technology/","section":"categories","tags":null,"title":"Technology"},{"body":"Setting up Cline and Ollama with Visual Studio Code As developers, we're always looking for ways to enhance our coding workflow with AI assistance. Today, we'll walk through setting up Cline (an AI coding assistant) with Ollama (local LLM runtime) in Visual Studio Code. This powerful combination gives you AI-powered coding assistance while keeping everything running locally on your machine. And yeah It's FREE :)\nWhat You'll Need Before we dive in, make sure you have:\nVisual Studio Code installed Node.js (version 18 or higher) At least 8GB of RAM (16GB recommended for larger models) A decent GPU (optional but recommended for faster inference) Step 1: Installing Ollama On macOS 1# Install via Homebrew 2brew install ollama 3 4# Or download directly from ollama.ai 5curl -fsSL https://ollama.ai/install.sh | sh On Linux 1# Install script 2curl -fsSL https://ollama.ai/install.sh | sh 3 4# Or via package managers 5# Ubuntu/Debian 6sudo apt install ollama 7 8# Arch Linux 9yay -S ollama On Windows Download the installer from ollama.ai Run the installer and follow the setup wizard Restart your terminal/PowerShell Verify Installation 1ollama --version Step 2: Setting up Ollama Models Start the Ollama service:\n1ollama serve In a new terminal, download and run your preferred model. Here are some popular options:\nFor Code Generation (Recommended) 1# CodeLlama - Excellent for code completion and generation 2ollama pull codellama:7b 3 4# DeepSeek Coder - Specifically trained for coding tasks 5ollama pull deepseek-coder:6.7b 6 7# Phind CodeLlama - Optimized for code explanation 8ollama pull phind-codellama:34b For General Purpose 1# Llama 2 - Good balance of capability and speed 2ollama pull llama2:7b 3 4# Mistral - Fast and capable 5ollama pull mistral:7b 6 7# Code-specific Mistral variant 8ollama pull codestral:22b Test your model:\n1ollama run codellama:7b Step 3: Installing Cline Extension Open Visual Studio Code Go to the Extensions view (Ctrl+Shift+X or Cmd+Shift+X) Search for \u0026quot;Cline\u0026quot; Install the Cline extension by the official publisher Restart VS Code Alternatively, install via command line:\n1code --install-extension cline.cline Step 4: Configuring Cline with Ollama Access Cline Settings Open VS Code Command Palette (Ctrl+Shift+P or Cmd+Shift+P) Type \u0026quot;Cline: Open Settings\u0026quot; and select it Or navigate to Settings â†’ Extensions â†’ Cline Configure Ollama Connection In the Cline settings:\n1{ 2 \u0026#34;cline.provider\u0026#34;: \u0026#34;ollama\u0026#34;, 3 \u0026#34;cline.ollama.baseUrl\u0026#34;: \u0026#34;http://localhost:11434\u0026#34;, 4 \u0026#34;cline.ollama.model\u0026#34;: \u0026#34;codellama:7b\u0026#34;, 5 \u0026#34;cline.temperature\u0026#34;: 0.3, 6 \u0026#34;cline.maxTokens\u0026#34;: 2048, 7 \u0026#34;cline.contextWindow\u0026#34;: 4096 8} Key Configuration Options Provider Settings:\nprovider: Set to \u0026quot;ollama\u0026quot; baseUrl: Ollama's default URL (usually http://localhost:11434) model: The model name you downloaded (e.g., \u0026quot;codellama:7b\u0026quot;) Performance Tuning:\ntemperature: Controls randomness (0.1-0.5 for code, 0.7-1.0 for creative tasks) maxTokens: Maximum response length contextWindow: How much previous conversation to remember Alternative Configuration via VS Code Settings UI File â†’ Preferences â†’ Settings Search for \u0026quot;Cline\u0026quot; Configure the following: Cline: Provider â†’ \u0026quot;ollama\u0026quot; Cline: Ollama Base URL â†’ \u0026quot;http://localhost:11434\u0026quot; Cline: Ollama Model â†’ \u0026quot;codellama:7b\u0026quot; (or your preferred model) Step 5: Testing Your Setup Basic Functionality Test Open a new file in VS Code (e.g., test.py) Open Command Palette (Ctrl+Shift+P) Run \u0026quot;Cline: Start Chat\u0026quot; or use the Cline panel Try a simple request: \u0026quot;Write a Python function to calculate fibonacci numbers\u0026quot; Troubleshooting Common Issues Cline can't connect to Ollama:\n1# Check if Ollama is running 2ps aux | grep ollama 3 4# Restart Ollama service 5ollama serve 6 7# Check port availability 8netstat -an | grep 11434 Model not found error:\n1# List available models 2ollama list 3 4# Pull the model if missing 5ollama pull codellama:7b Slow responses:\nTry smaller models (7B instead of 13B/34B) Reduce context window size Close other memory-intensive applications Step 6: Optimizing Your Workflow Recommended Model Selection by Use Case For Fast Code Completion:\ncodellama:7b - Quick responses, good for autocompletion deepseek-coder:1.3b - Ultra-fast, basic code assistance For Complex Code Analysis:\ncodellama:13b - Better understanding, slower responses phind-codellama:34b - Excellent for debugging and explanations For General Programming Help:\nmistral:7b - Good balance of speed and capability codestral:22b - Specialized for code tasks Performance Tips System Optimization:\n1# Increase Ollama\u0026#39;s memory allocation 2export OLLAMA_HOST=127.0.0.1:11434 3export OLLAMA_MAX_LOADED_MODELS=2 4export OLLAMA_NUM_PARALLEL=2 VS Code Settings:\n1{ 2 \u0026#34;cline.autoSuggest\u0026#34;: true, 3 \u0026#34;cline.inlineCompletion\u0026#34;: true, 4 \u0026#34;cline.debounceTime\u0026#34;: 300, 5 \u0026#34;cline.enableFileContext\u0026#34;: true 6} Step 7: Advanced Configuration Custom Model Parameters Create an Ollama Modelfile for fine-tuned behavior:\n1FROM codellama:7b 2 3# Set custom parameters 4PARAMETER temperature 0.2 5PARAMETER top_k 40 6PARAMETER top_p 0.9 7PARAMETER repeat_penalty 1.1 8 9# Custom system prompt for coding 10SYSTEM \u0026#34;\u0026#34;\u0026#34; 11You are a helpful coding assistant. Provide concise, accurate code solutions. 12Focus on clean, readable code with appropriate comments. 13\u0026#34;\u0026#34;\u0026#34; Apply the custom model:\n1ollama create my-code-assistant -f ./Modelfile 2ollama run my-code-assistant Update Cline to use the custom model:\n1{ 2 \u0026#34;cline.ollama.model\u0026#34;: \u0026#34;my-code-assistant\u0026#34; 3} Project-Specific Settings Create .vscode/settings.json in your project:\n1{ 2 \u0026#34;cline.provider\u0026#34;: \u0026#34;ollama\u0026#34;, 3 \u0026#34;cline.ollama.model\u0026#34;: \u0026#34;deepseek-coder:6.7b\u0026#34;, 4 \u0026#34;cline.temperature\u0026#34;: 0.1, 5 \u0026#34;cline.enableProjectContext\u0026#34;: true, 6 \u0026#34;cline.includeFileTypes\u0026#34;: [\u0026#34;.js\u0026#34;, \u0026#34;.ts\u0026#34;, \u0026#34;.py\u0026#34;, \u0026#34;.go\u0026#34;, \u0026#34;.rs\u0026#34;] 7} Usage Examples Code Generation Prompt: \u0026quot;Create a React component for a user profile card\u0026quot;\nResult: Cline will generate a complete React component with props, styling, and proper structure.\nCode Explanation Prompt: \u0026quot;Explain this regex pattern: ^(?:[a-z0-9!#$%\u0026amp;'*+/=?^_{|}~-]+(?:.[a-z0-9!#$%\u0026amp;'*+/=?^_{|}~-]+)*|\u0026quot;(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\u0026quot;)@\u0026quot;\nResult: Cline will break down the regex and explain each component clearly.\nDebugging Assistance Prompt: \u0026quot;This function is throwing a null pointer exception, can you help debug it?\u0026quot;\nResult: Cline will analyze your code and suggest potential fixes.\nBest Practices Model Management 1# Keep only models you actively use 2ollama list 3ollama rm unused-model:tag 4 5# Update models regularly 6ollama pull codellama:7b Resource Monitoring 1# Monitor Ollama resource usage 2ollama ps 3 4# Check GPU utilization (if using GPU) 5nvidia-smi Backup Your Configuration Save your working Cline configuration:\n1# Export VS Code settings 2cp ~/.config/Code/User/settings.json ~/cline-backup-settings.json Troubleshooting Guide Common Issues and Solutions Issue: \u0026quot;Connection refused\u0026quot; error\n1# Solution: Ensure Ollama is running 2ollama serve \u0026amp; Issue: Model responses are too slow\nSwitch to a smaller model (7B instead of 13B) Reduce context window size Close unnecessary applications Issue: Out of memory errors\n1# Solution: Limit concurrent requests 2export OLLAMA_MAX_LOADED_MODELS=1 Issue: Cline suggestions are irrelevant\nAdjust temperature (lower for more focused responses) Update your system prompt Ensure you're using a code-specific model Security Considerations Since everything runs locally:\nâœ… Your code never leaves your machine âœ… No API keys or external services required âœ… Complete privacy and data control âœ… Works offline once models are downloaded Performance Benchmarks Based on typical setups:\nModel Size Speed Quality Best For codellama:7b ~4GB Fast Good General coding deepseek-coder:6.7b ~4GB Fast Excellent Code-specific tasks codellama:13b ~8GB Medium Better Complex analysis phind-codellama:34b ~20GB Slow Excellent Detailed explanations Conclusion Setting up Cline with Ollama gives you a powerful, privacy-focused coding assistant that runs entirely on your local machine. The combination provides:\nLocal AI assistance without cloud dependencies Customizable models for different coding tasks Privacy and security with no data sharing Cost-effective solution with no API fees Start with a smaller model like codellama:7b to test your setup, then experiment with larger models based on your hardware capabilities and specific needs.\nNext Steps Explore different models for various programming languages Create custom Ollama models fine-tuned for your specific projects Set up project-specific Cline configurations Integrate with your existing development workflow Happy coding! ðŸš€\nHave questions or run into issues? The Cline and Ollama communities are very helpful. Check out their respective GitHub repositories and Discord servers for support.\n","link":"https://peerasan.github.io/post/2025/cline-ollama-vscode-setup/","section":"post","tags":["ai","vibe-coding","cline","ollama"],"title":"Using Cline and Ollama with Visual Studio Code"},{"body":"","link":"https://peerasan.github.io/tags/vibe-coding/","section":"tags","tags":null,"title":"Vibe-Coding"},{"body":"","link":"https://peerasan.github.io/tags/audio/","section":"tags","tags":null,"title":"Audio"},{"body":"","link":"https://peerasan.github.io/tags/video/","section":"tags","tags":null,"title":"Video"},{"body":"Whisper: Audio Transcription and Subtitle Extraction Overview Let's try to use FFmpeg in combination with OpenAI's Whisper to extract transcriptions and generate subtitles from audio and video files. FFmpeg handles media processing while Whisper provides state-of-the-art speech recognition.\nPrerequisites Required Software FFmpeg: Media processing framework Whisper: OpenAI's automatic speech recognition system Python 3.7+: Required for Whisper Installation Install FFmpeg 1# Windows (using chocolatey) 2choco install ffmpeg 3 4# macOS (using homebrew) 5brew install ffmpeg 6 7# Ubuntu/Debian 8sudo apt update 9sudo apt install ffmpeg 10 11# Verify installation 12ffmpeg -version Install Whisper 1# Install via pip 2pip install openai-whisper 3 4# Or install the latest development version 5pip install git+https://github.com/openai/whisper.git 6 7# Verify installation 8whisper --help Basic Workflow Method 1: Direct Audio/Video to Text For files that Whisper can process directly:\n1# Basic transcription 2whisper audio_file.mp3 3 4# Specify output format 5whisper video_file.mp4 --output_format txt 6 7# Choose model size (tiny, base, small, medium, large) 8whisper audio_file.wav --model medium 9 10# Specify language (optional, auto-detected by default) 11whisper audio_file.mp3 --language Thai Method 2: FFmpeg + Whisper Pipeline For better control or problematic file formats:\nStep 1: Extract Audio with FFmpeg 1# Extract audio as WAV (recommended for Whisper) 2ffmpeg -i input_video.mp4 -vn -acodec pcm_s16le -ar 16000 -ac 1 audio_output.wav 3 4# Extract audio as MP3 5ffmpeg -i input_video.mkv -vn -acodec mp3 -ab 128k audio_output.mp3 6 7# Extract specific time range 8ffmpeg -i input.mp4 -ss 00:01:30 -t 00:02:00 -vn -acodec pcm_s16le audio_segment.wav Step 2: Transcribe with Whisper 1# Basic transcription 2whisper audio_output.wav 3 4# Generate multiple formats simultaneously 5whisper audio_output.wav --output_format txt,srt,vtt,json 6 7# Use specific model and language 8whisper audio_output.wav --model large --language en --output_format srt Output Formats Available Output Formats txt: Plain text transcription srt: SubRip subtitle format vtt: WebVTT subtitle format json: Detailed JSON with timestamps and confidence scores tsv: Tab-separated values with timestamps Examples 1# Generate SRT subtitles 2whisper video.mp4 --output_format srt 3 4# Generate multiple formats 5whisper audio.wav --output_format txt,srt,vtt,json 6 7# Custom output directory 8whisper audio.mp3 --output_dir ./transcriptions --output_format srt Advanced FFmpeg Preprocessing Audio Quality Optimization 1# Normalize audio levels 2ffmpeg -i input.mp4 -af \u0026#34;dynaudnorm\u0026#34; -acodec pcm_s16le normalized_audio.wav 3 4# Remove background noise (basic) 5ffmpeg -i input.mp4 -af \u0026#34;highpass=f=200,lowpass=f=3000\u0026#34; filtered_audio.wav 6 7# Boost volume 8ffmpeg -i input.mp4 -af \u0026#34;volume=2.0\u0026#34; louder_audio.wav Handling Multiple Audio Tracks 1# List audio streams 2ffmpeg -i input.mkv 3 4# Extract specific audio track (e.g., track 1) 5ffmpeg -i input.mkv -map 0:a:1 -vn audio_track2.wav 6 7# Mix multiple audio tracks 8ffmpeg -i input.mkv -af \u0026#34;amix=inputs=2\u0026#34; mixed_audio.wav Batch Processing 1# Process multiple files (bash) 2for file in *.mp4; do 3 ffmpeg -i \u0026#34;$file\u0026#34; -vn -acodec pcm_s16le \u0026#34;${file%.*}.wav\u0026#34; 4 whisper \u0026#34;${file%.*}.wav\u0026#34; --output_format srt 5done 6 7# Windows batch processing 8for %f in (*.mp4) do ( 9 ffmpeg -i \u0026#34;%f\u0026#34; -vn -acodec pcm_s16le \u0026#34;%~nf.wav\u0026#34; 10 whisper \u0026#34;%~nf.wav\u0026#34; --output_format srt 11) Whisper Model Selection Available Models Model Parameters English-only Multilingual Required VRAM Relative Speed tiny 39 M âœ“ âœ“ ~1 GB ~32x base 74 M âœ“ âœ“ ~1 GB ~16x small 244 M âœ“ âœ“ ~2 GB ~6x medium 769 M âœ“ âœ“ ~5 GB ~2x large 1550 M âœ— âœ“ ~10 GB 1x Model Selection Guidelines 1# Fast transcription (lower accuracy) 2whisper audio.wav --model tiny 3 4# Balanced speed/accuracy 5whisper audio.wav --model base 6 7# High accuracy (slower) 8whisper audio.wav --model large 9 10# English-only models (slightly better for English) 11whisper audio.wav --model base.en Common Use Cases 1. YouTube Video Transcription 1# Download with yt-dlp, extract audio, transcribe 2yt-dlp -x --audio-format mp3 \u0026#34;https://youtube.com/watch?v=VIDEO_ID\u0026#34; 3whisper \u0026#34;VIDEO_TITLE.mp3\u0026#34; --output_format srt 4 5# Or direct processing if file is compatible 6whisper \u0026#34;$(yt-dlp --get-filename -x --audio-format mp3 \u0026#39;VIDEO_URL\u0026#39;)\u0026#34; --output_format srt 2. Podcast Episode Processing 1# Extract and normalize audio 2ffmpeg -i podcast_episode.mp3 -af \u0026#34;dynaudnorm\u0026#34; -ar 16000 normalized_podcast.wav 3 4# Transcribe with timestamps 5whisper normalized_podcast.wav --output_format json,srt --model medium 3. Meeting Recording Transcription 1# Extract audio from meeting recording 2ffmpeg -i meeting_recording.mp4 -vn -ar 16000 -ac 1 meeting_audio.wav 3 4# Transcribe with word-level timestamps 5whisper meeting_audio.wav --output_format json --model base --word_timestamps True 4. Foreign Language Content 1# Auto-detect language 2whisper foreign_audio.mp3 --model medium --output_format srt 3 4# Specify language for better accuracy 5whisper thai_audio.mp3 --language Thai --model medium --output_format srt 6 7# Translate to English while transcribing 8whisper foreign_audio.mp3 --task translate --output_format srt Optimization Tips Performance Optimization 1# Use GPU acceleration (if available) 2whisper audio.wav --device cuda 3 4# Specify number of CPU threads 5whisper audio.wav --threads 4 6 7# Use faster models for real-time processing 8whisper audio.wav --model tiny.en --fp16 False Quality Optimization 1# Improve accuracy with audio preprocessing 2ffmpeg -i noisy_audio.mp3 -af \u0026#34;highpass=f=80,lowpass=f=8000,dynaudnorm\u0026#34; clean_audio.wav 3whisper clean_audio.wav --model large --temperature 0 4 5# Use temperature for consistency 6whisper audio.wav --temperature 0 --best_of 5 Troubleshooting Common Issues File Format Problems 1# Convert unsupported formats 2ffmpeg -i input.webm -acodec pcm_s16le -ar 16000 output.wav 3ffmpeg -i input.flac -acodec mp3 output.mp3 Memory Issues 1# Use smaller model 2whisper large_file.wav --model tiny 3 4# Process in segments 5ffmpeg -i large_file.mp4 -f segment -segment_time 300 -vn segment_%03d.wav Poor Transcription Quality 1# Try different models 2whisper audio.wav --model medium # vs base or large 3 4# Specify language explicitly 5whisper audio.wav --language en 6 7# Adjust audio quality 8ffmpeg -i input.mp3 -af \u0026#34;volume=2.0,highpass=f=100\u0026#34; enhanced.wav Advanced Features Custom Vocabulary and Prompts 1# Use initial prompt for context 2whisper audio.wav --initial_prompt \u0026#34;This is a technical discussion about machine learning\u0026#34; 3 4# For names and technical terms 5whisper audio.wav --initial_prompt \u0026#34;Speakers: Peerasan Buranasanti. Topic: Media Streaming\u0026#34; Fine-tuning Output 1# Suppress non-speech tokens 2whisper audio.wav --suppress_tokens \u0026#34;50257\u0026#34; 3 4# No speech threshold (adjust for silence detection) 5whisper audio.wav --no_speech_threshold 0.6 6 7# Compression ratio threshold 8whisper audio.wav --compression_ratio_threshold 2.4 Integration with Video Editing 1# Generate subtitle file for video editing 2whisper video.mp4 --output_format srt --model medium 3 4# Burn subtitles directly into video 5whisper video.mp4 --output_format srt 6ffmpeg -i video.mp4 -vf \u0026#34;subtitles=th-video.srt\u0026#34; video_with_subs.mp4 Complete Example Workflows Workflow 1: Conference Talk Processing 1#!/bin/bash 2# conference_transcription.sh 3 4INPUT_FILE=\u0026#34;$1\u0026#34; 5OUTPUT_NAME=\u0026#34;${INPUT_FILE%.*}\u0026#34; 6 7echo \u0026#34;Processing: $INPUT_FILE\u0026#34; 8 9# Step 1: Extract and optimize audio 10ffmpeg -i \u0026#34;$INPUT_FILE\u0026#34; \\ 11 -vn \\ 12 -af \u0026#34;dynaudnorm,highpass=f=80,lowpass=f=8000\u0026#34; \\ 13 -ar 16000 \\ 14 -ac 1 \\ 15 \u0026#34;${OUTPUT_NAME}_processed.wav\u0026#34; 16 17# Step 2: Transcribe with Whisper 18whisper \u0026#34;${OUTPUT_NAME}_processed.wav\u0026#34; \\ 19 --model medium \\ 20 --output_format txt,srt,json \\ 21 --language Thai \\ 22 --initial_prompt \u0026#34;This is a technical conference presentation\u0026#34; 23 24# Step 3: Clean up 25rm \u0026#34;${OUTPUT_NAME}_processed.wav\u0026#34; 26 27echo \u0026#34;Transcription complete: ${OUTPUT_NAME}.txt, ${OUTPUT_NAME}.srt\u0026#34; Workflow 2: Multilingual Content 1#!/bin/bash 2# multilingual_transcription.sh 3 4INPUT_FILE=\u0026#34;$1\u0026#34; 5 6# Extract audio 7ffmpeg -i \u0026#34;$INPUT_FILE\u0026#34; -vn -ar 16000 temp_audio.wav 8 9# Detect language and transcribe 10whisper temp_audio.wav --model medium --output_format json 11 12# Also create English translation 13whisper temp_audio.wav --task translate --output_format srt --model medium 14 15# Clean up 16rm temp_audio.wav Best Practices Audio Quality Guidelines Sample Rate: 16kHz is optimal for Whisper Format: WAV or high-quality MP3 Mono vs Stereo: Mono is sufficient for speech Bit Depth: 16-bit is adequate Model Selection Strategy tiny/base: Quick drafts, real-time processing small/medium: Balanced accuracy/speed for most use cases large: Maximum accuracy for important content File Organization 1project/ 2â”œâ”€â”€ original_files/ 3â”œâ”€â”€ processed_audio/ 4â”œâ”€â”€ transcriptions/ 5â”‚ â”œâ”€â”€ txt/ 6â”‚ â”œâ”€â”€ srt/ 7â”‚ â””â”€â”€ json/ 8â””â”€â”€ scripts/ Useful FFmpeg Commands for Whisper Prep Audio Extraction and Conversion 1# Extract best quality audio 2ffmpeg -i input.mkv -q:a 0 -map a output.mp3 3 4# Convert to Whisper-optimal format 5ffmpeg -i input.mp4 -vn -ar 16000 -ac 1 -c:a pcm_s16le output.wav 6 7# Extract audio from specific timeframe 8ffmpeg -i input.mp4 -ss 00:10:00 -to 00:20:00 -vn output.wav Audio Enhancement 1# Normalize and filter 2ffmpeg -i input.mp3 -af \u0026#34;dynaudnorm=f=75:g=25,highpass=f=80,lowpass=f=8000\u0026#34; clean.wav 3 4# Reduce noise (basic) 5ffmpeg -i noisy.wav -af \u0026#34;afftdn\u0026#34; denoised.wav Performance Monitoring Check Processing Time 1# Time the transcription 2time whisper audio.wav --model base 3 4# Monitor GPU usage (if using CUDA) 5nvidia-smi Batch Processing with Progress 1#!/bin/bash 2total_files=$(ls *.mp4 | wc -l) 3current=0 4 5for file in *.mp4; do 6 ((current++)) 7 echo \u0026#34;Processing file $current of $total_files: $file\u0026#34; 8 9 ffmpeg -i \u0026#34;$file\u0026#34; -vn -ar 16000 temp.wav 10 whisper temp.wav --output_format srt --model base 11 rm temp.wav 12 13 echo \u0026#34;Completed: ${file%.*}.srt\u0026#34; 14done Integration Tips With Video Editors Export SRT files for Premiere Pro, DaVinci Resolve, etc. Use VTT format for web players JSON format provides detailed timing for custom applications With Automation Scripts 1# Python automation example 2import subprocess 3import os 4 5def process_video(input_path, model=\u0026#34;base\u0026#34;): 6 # Extract audio 7 audio_path = f\u0026#34;{os.path.splitext(input_path)[0]}.wav\u0026#34; 8 subprocess.run([ 9 \u0026#34;ffmpeg\u0026#34;, \u0026#34;-i\u0026#34;, input_path, \u0026#34;-vn\u0026#34;, 10 \u0026#34;-ar\u0026#34;, \u0026#34;16000\u0026#34;, \u0026#34;-ac\u0026#34;, \u0026#34;1\u0026#34;, audio_path 11 ]) 12 13 # Transcribe 14 subprocess.run([ 15 \u0026#34;whisper\u0026#34;, audio_path, \u0026#34;--model\u0026#34;, model, 16 \u0026#34;--output_format\u0026#34;, \u0026#34;srt,txt\u0026#34; 17 ]) 18 19 # Clean up 20 os.remove(audio_path) Troubleshooting Common Issues and Solutions Issue Solution \u0026quot;No module named whisper\u0026quot; pip install openai-whisper FFmpeg not found Add FFmpeg to system PATH Poor transcription quality Try larger model, improve audio quality Out of memory Use smaller model or process shorter segments Wrong language detected Specify --language parameter No timestamps in output Use SRT, VTT, or JSON format Audio Quality Issues 1# Check audio properties 2ffprobe -v quiet -show_format -show_streams input.mp4 3 4# Test with sample 5ffmpeg -i input.mp4 -t 30 -vn sample.wav 6whisper sample.wav --model base Quick Reference Commands Essential Commands 1# Basic transcription 2whisper file.mp4 3 4# High-quality subtitles 5whisper file.mp4 --model large --output_format srt 6 7# Extract audio + transcribe 8ffmpeg -i video.mp4 -vn audio.wav \u0026amp;\u0026amp; whisper audio.wav --output_format srt 9 10# Batch process current directory 11for f in *.mp4; do whisper \u0026#34;$f\u0026#34; --output_format srt; done 12 13# Foreign language with translation 14whisper foreign.mp3 --task translate --output_format srt Useful FFmpeg Audio Processing 1# Optimize for speech recognition 2ffmpeg -i input.mp4 -af \u0026#34;dynaudnorm,highpass=f=80,lowpass=f=8000\u0026#34; -ar 16000 -ac 1 optimized.wav 3 4# Split long audio into segments 5ffmpeg -i long_audio.mp3 -f segment -segment_time 600 -c copy segment_%03d.mp3 6 7# Merge multiple audio files 8ffmpeg -f concat -safe 0 -i filelist.txt -c copy merged.mp3 Note: Processing time varies significantly based on audio length, model size, and hardware. The large model provides the best accuracy but requires substantial computational resources.\n","link":"https://peerasan.github.io/post/2025/whisper/","section":"post","tags":["audio","video","ai","whisper"],"title":"Whisper"},{"body":"","link":"https://peerasan.github.io/tags/whisper/","section":"tags","tags":null,"title":"Whisper"},{"body":"","link":"https://peerasan.github.io/categories/archives/","section":"categories","tags":null,"title":"Archives"},{"body":"","link":"https://peerasan.github.io/tags/av1/","section":"tags","tags":null,"title":"Av1"},{"body":"WebRTC: End-to-End ~150ms\nOBS using for create virtual webcam Browser send video H.264/Opus to Server in Singapore Video playback (Thailand) ","link":"https://peerasan.github.io/post/archives/libaom-av1-compare/","section":"post","tags":["av1","video","lowlatency","webrtc","streaming"],"title":"av1 compare"},{"body":" ","link":"https://peerasan.github.io/post/archives/compile-ffmpeg-with-aom-av1-and-svt-av1/","section":"post","tags":["av1","video","ffmpeg"],"title":"Compile FFmpeg with AOM AV1 and SVT-AV1"},{"body":"","link":"https://peerasan.github.io/tags/ffmpeg/","section":"tags","tags":null,"title":"Ffmpeg"},{"body":"","link":"https://peerasan.github.io/tags/lowlatency/","section":"tags","tags":null,"title":"Lowlatency"},{"body":"\u0026quot;Secure Reliable Transport (SRT) is an open source video transport protocol that utilises the UDP transport protocol\u0026quot; (wiki)\nFeature UDP is faster TCP Lost packet? SRT can retransmit lost packets quickly AES encryption Transport protocol, so you can use any video format, high compression codec like HEVC, AV1 (Also LCEVC, AV2, VVC in the future) Low latency ~1000ms SRT test on local network I install OBS, ffmpeg and srt-live-transmit on localhost. latency results around 300ms.\nSRT End-to-End testing Now, I use OBS to stream video to the SRT server at Singapore (RTT from Thailand around ~30ms) then playback from Thailand. The latency is around ~400ms which is not bad. However I have not optimized much, so may be a better result if I optimize all factors.\nSRT (Secure Reliable Transport) Latency (End-to-End) #1 OBS for Video Encoder (Thailand) SRT Server (Singapore) Playback (Thailand) Latency ~532ms SRT (Secure Reliable Transport) Latency (End-to-End) #2 OBS for Video Encoder (Thailand) SRT Server (Singapore) Playback (Thailand) Latency ~365ms SRT (Secure Reliable Transport) Latency (End-to-End) #3 OBS for Video Encoder (Thailand) SRT Server (Singapore) Playback (Thailand) Latency ~400ms What I Config Please note that I test on my low spec desktop (AMD 3200g). I believe that the video encoder process is a key factor of the result. Better computer performance should better result.\n","link":"https://peerasan.github.io/post/archives/srt-e2e/","section":"post","tags":["av1","video","ffmpeg"],"title":"SRT (Secure Reliable Transport) Latency (End-to-End)"},{"body":"","link":"https://peerasan.github.io/tags/streaming/","section":"tags","tags":null,"title":"Streaming"},{"body":"WebRTC: End-to-End ~150ms\nOBS using for create virtual webcam Browser send video H.264/Opus to Server in Singapore Video playback (Thailand) ","link":"https://peerasan.github.io/post/archives/webrtc/","section":"post","tags":["av1","video","lowlatency","webrtc","streaming"],"title":"Webrtc"},{"body":"","link":"https://peerasan.github.io/tags/webrtc/","section":"tags","tags":null,"title":"Webrtc"},{"body":" ","link":"https://peerasan.github.io/post/archives/webrtc-with-av1/","section":"post","tags":["av1","video","lowlatency","webrtc","streaming"],"title":"Webrtc With AV1"},{"body":"","link":"https://peerasan.github.io/tags/computervision/","section":"tags","tags":null,"title":"ComputerVision"},{"body":"","link":"https://peerasan.github.io/tags/deeplearning/","section":"tags","tags":null,"title":"Deeplearning"},{"body":"","link":"https://peerasan.github.io/tags/facemesh/","section":"tags","tags":null,"title":"FaceMesh"},{"body":"","link":"https://peerasan.github.io/tags/imageprocessing/","section":"tags","tags":null,"title":"ImageProcessing"},{"body":"","link":"https://peerasan.github.io/tags/machinelearning/","section":"tags","tags":null,"title":"Machinelearning"},{"body":"","link":"https://peerasan.github.io/tags/posenet/","section":"tags","tags":null,"title":"PoseNet"},{"body":" ","link":"https://peerasan.github.io/post/archives/realtime-cartoon-animation-from-tiktok/","section":"post","tags":["Tensorflow","ImageProcessing","VideoProcessing","machinelearning","deeplearning","AI","PoseNet","FaceMesh","ComputerVision"],"title":"Realtime Cartoon Animation From Tiktok"},{"body":"","link":"https://peerasan.github.io/tags/tensorflow/","section":"tags","tags":null,"title":"Tensorflow"},{"body":"","link":"https://peerasan.github.io/tags/videoprocessing/","section":"tags","tags":null,"title":"VideoProcessing"},{"body":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\nhttps://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremely fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","link":"https://peerasan.github.io/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://peerasan.github.io/series/","section":"series","tags":null,"title":"Series"}]